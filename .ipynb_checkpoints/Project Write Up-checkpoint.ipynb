{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ratings from Locations and Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Table of Contents\n",
    "* [Analysis of Film Locations](#Analysis-of-Film-Locations)\n",
    "    * [1. Overview](#1.-Overview)\n",
    "\t* [2. Data Acquisition & Management](#2.-Data-Acquisition-&-Management)\n",
    "        * [2.1. Approach 1](#2.1.-Approach-1)\n",
    "        * [2.2. Approach 2](#2.2.-Approach-2) \n",
    "    * [3. Statistics](#3.-Statistics)\n",
    "        * [3.1. Summary Statistics of Data Set](#3.1.-Summary-Statistics-of-Data-Set)\n",
    "        * [3.2. Principal Component Analysis](#3.2.-Principal-Component-Analysis)\n",
    "        * [3.3. Train Test Splitting](#3.3.-Train-Test-Splitting)\n",
    "        * [3.4. Logistic Regression](#3.4.-Logistic-Regression)\n",
    "        * [3.5. K-Nearest Neighbours](#3.5.-K-Nearest-Neighbours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we study the relationship between film locations and other factors like ratings and genres. Data will be scraped from [IMDB](http://www.imdb.com). Statistical and machine learning analytical tools will be used in our study. Visualisation of results will be done via graphical plots and Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Data Acquisition & Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first scrape the data from IMDB which gives us information about each movie such as budget, country, critic ratings, duration, genre, gross earnings, language, location, name, opening weekend earnings, release dates, url, user ratings, user ratings count and year. Though we wanted a data set of size 12,000 initially, we had to lower that number to keep our project feasible. So, we focus on 6,000 random samples of the most recent titles. As an example, there are 8,997 titles in 2014. However, some of these are missing location data. Hence, we scraped as far back as we could to obtain the desired data set size.\n",
    "\n",
    "The ratings and genres of each title does not require much cleaning. We merely remove titles with no ratings. However, because the locations of each title is user-contributed, it requires much cleaning. For example, there are many cases of multiple similar entries, albeit differently worded. In some cases, locations are specific to the street while others merely state the country. Our first task is to clean the locations variable. After which, we split our data into two sets for training and testing in the ratio 3:1.\n",
    "\n",
    "In cleaning up the variable \"locations\", there are two approaches for consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.1. Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we restrict \"locations\" to cities; in particular, cities belonging to top filming locations. In this case, the top $T$ filming locations will be identified by the most frequent filming locations in our data set. So, we split each \"locations\" by commas into phrases and add them to a set. Then, we remove elements of this set which are not cities, as defined by [Wikipedia](https://en.wikipedia.org/wiki/List_of_towns_and_cities_with_100,000_or_more_inhabitants/country:_A-B). Then, we find the $T$ most frequent elements from our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2. Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the entries of \"locations\" by commas and treat each unique phrase as a distinct location. For example, suppose we have \"Newbury Street, Boston, USA, Boston, USA\". We split them into \"Newbury Street\", \"Boston\" and \"USA\". Later on, in our analysis, we treat each of these as distinct locations.\n",
    "\n",
    "We will proceed with this approach so as to avoid mistakably introducing bias into our analysis. PCA will be used to reduce the number of locations and genres later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.1. Summary Statistics of Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a better idea of the data set that we are working with, it helps to have some plots of our data. We can look at a histogram of the ratings. Depending on the skewness of ratings, we can categorise ratings based on a threshold into a binary variable \"Category\". Perhaps, if half of the titles have ratings above 7.5, we can categorise titles with ratings above 7.5 as \"Good\" and those below as \"Bad\".\n",
    "\n",
    "We can also look at bar plots of the number of titles for each city and each genre. This would give us some sense of the difference in numbers for each city and genre. Potentially, we might want to look at the relation between cities and genres. For example, what is the probability that a title was filmed in New York conditional on it being a drama. We might be able to write a function where you input parameters $X$ and $Y$, each representing a city or genre, and it returns $Probability(X|Y)$.\n",
    "\n",
    "We can present some of these results using Tableau. For example, we can present the number of titles per city on a map in Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.2. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin analysis by using PCA to reduce the number of dimensions of our independent variables in the data. Our variation cutoff will be 5%; so if variation falls below 5% after a certain component, we will remove all remaining dimensions. Then, we split our data set into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Replace 60 with desired n later\n",
    "pca = PCA(n_components=60)\n",
    "\n",
    "#X should consist only of independent variables, leave out Y\n",
    "new_X = pca.fit_transform(df[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "print pca.explained_variance_ratio_.sum() #This should be above 95% at desired n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loop to find the optimal n_components that explains 95% of variation\n",
    "\n",
    "for n in range(1,1001):\n",
    "        pca = PCA(n_components=n)\n",
    "        new_X = pca.fit_transform(df[X])\n",
    "        if pca.explained_variance_ratio_.sum()>0.95:\n",
    "            N_pca = n\n",
    "            print \"Optimal n:\", n\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply N_pca to data set\n",
    "pca = PCA(n_components=N_pca)\n",
    "\n",
    "new_X = pca.fit_transform(df[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The N components\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the optimal n components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.3. Train Test Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split our data set into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(df[X], df[Y], train_size=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to study the effects of locations and genre on category. We denote $ Y = $ Category, $L_c$ = Dummy Variable for City $c$, $G_g$ = Dummy Variable for Genre $g$.\n",
    "\n",
    "Our regression equation will take the following form:\n",
    "\n",
    "$$ P(Y_i=1) = F(\\beta_0 + \\delta_1 L_{1i} + \\delta_2 L_{2i} + ... + \\delta_C L_{Ci} + \\gamma_1 G_{1i} + \\gamma_2 G_{2i} + ... + \\gamma_G G_{Gi})$$\n",
    "\n",
    "where $\\beta_0$ is the intercept parameter, $\\delta_c$ are the slope parameters for the locations, $\\gamma_g$ are the slope parameters for the genres and $F$ is the logistic function defined as:\n",
    "\n",
    "$$ F(x) = \\frac{e^x}{e^x + 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After estimating the coefficients, we apply the coefficient estimates on the test set. Then, we compare the accuracy of predicting $Y$ on both the training and test set. If the accuracy on the training set is very high while the accuracy on the test set is very low, it suggests that our model is overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we first find the best paramater C for the logistic regression from the training set using n-folds cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf=LogisticRegression()\n",
    "parameters = {\"C\": [0.0001, 0.001, 0.1, 1, 10, 100]}\n",
    "fitmodel = GridSearchCV(clf, param_grid=parameters, cv=5, scoring=\"accuracy\")\n",
    "fitmodel.fit(Xlr, ylr)\n",
    "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we perform the logistic regression using the best parameter C, fit our training set and then test on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfl2=LogisticRegression(C=fitmodel.best_params_['C'])\n",
    "clfl2.fit(Xlr, ylr)\n",
    "ypred2=clfl2.predict(Xtestlr)\n",
    "accuracy_score(ypred2, ytestlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3.5. K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the non-parametrised kNN classification with distance defined by the locations and genre. We train the data and use cross-validation to find the optimal K. Then, we look at the accuracy of prediction on the test set, comparing it with the training set. A significant difference would indicate overfitting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for performing kNN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_classify(X,y, nbrs, plotit=True):\n",
    "    clf= KNeighborsClassifier(nbrs)\n",
    "    clf=clf.fit(X, y)\n",
    "    accuracy = clf.score(X, y)\n",
    "    if plotit:\n",
    "        print \"Accuracy: %0.2f\" % (accuracy)\n",
    "        plt.figure()\n",
    "        ax=plt.gca()\n",
    "        points_plot(ax, Xtrain, Xtest, ytrain, ytest, clf, alpha=0.3, psize=20)\n",
    "    return nbrs, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to find the optimal k neighbours from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_nbrs(clf, parameters, Xtrain, ytrain, n_folds=5):\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    return gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply to the test set and compare the accuracy with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
